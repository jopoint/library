{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy \n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip,os,glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import librosa\n",
    "from sklearn import *\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_vector(dimensions,seed):\n",
    "    arr = np.array([0] * (dimensions-seed) + [1] * int(seed/2) + [-1] * int(seed/2) )\n",
    "    np.random.shuffle(arr)\n",
    "    return(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "textz=[]\n",
    "word_vec=dict()\n",
    "word_docz=dict()\n",
    "dimensions=500\n",
    "seed=8\n",
    "\n",
    "with open('../data/after_nltk/df_content_after_nltk+titles.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile) \n",
    "    for row in reader:\n",
    "        textz.append(row['content'].lower())\n",
    "        #if int(row['id'])>5000:\n",
    "        #    break\n",
    "        if int(row['id'])%1000==0:\n",
    "            print(row['id'])\n",
    "        current_doc_wordz=dict()\n",
    "        content=row['content'].lower()\n",
    "        for word in re.split('[\\W]', content):\n",
    "            if word not in current_doc_wordz:\n",
    "                current_doc_wordz[word]=1\n",
    "                try:\n",
    "                    word_docz[word]=word_docz[word]+1\n",
    "                except:\n",
    "                    word_docz[word]=1\n",
    "                    word_vec[word]=generate_random_vector(dimensions,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24213"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we focus on words which occur in more than HAPAX_THRESHOLD and less than STOPWORD_THRESHOLD documents because they just bring noise in our vector space\n",
    "\n",
    "HAPAX_THRESHOLD=4\n",
    "STOPWORD_THRESHOLD=1000\n",
    "nonsingular_wordz = dict((k, v) for k, v in word_docz.items() if v > HAPAX_THRESHOLD and v<STOPWORD_THRESHOLD)\n",
    "\n",
    "\n",
    "#we create IDF (inverse-document frequency) weights\n",
    "normalize_factor=1 / sum(nonsingular_wordz.values())\n",
    "for k in nonsingular_wordz:\n",
    "    nonsingular_wordz[k]*=normalize_factor\n",
    "    #print(nonsingular_wordz[k],k)\n",
    "    \n",
    "len(nonsingular_wordz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand word 2 doc\n",
      "\n",
      "doc 2 word\n",
      "word 2 doc\n"
     ]
    }
   ],
   "source": [
    "#0th pass - from random word vectors to meaningful doc vectors\n",
    "print(\"rand word 2 doc\\n\")\n",
    "doc_vec=np.zeros((len(textz),dimensions))\n",
    "line_id=0\n",
    "for text in textz:\n",
    "    for word in re.split(' ',text):\n",
    "        if word in nonsingular_wordz:\n",
    "            #document vector is a linear combination of IDF-weighted word vectors which document contains\n",
    "            doc_vec[line_id] = ( word_vec[word] / (nonsingular_wordz[word]) ) + doc_vec[line_id]\n",
    "            #simplified version (withoug IDF-weighting)\n",
    "            #doc_vec[line_id] = word_vec[word]   + doc_vec[line_id]\n",
    "    line_id=line_id+1\n",
    " \n",
    "\n",
    "for i in range(len(doc_vec)):\n",
    "    doc_vec[i]=preprocessing.normalize(doc_vec[i].reshape(1,-1))\n",
    "\n",
    "    \n",
    "#from document vectors to word vectors\n",
    "print(\"doc 2 word\")\n",
    "word_vec2=dict()\n",
    "line_id=0\n",
    "for text in textz:\n",
    "    for word in re.split(' ',text):\n",
    "        if word in nonsingular_wordz:\n",
    "            #print(word)\n",
    "            if word in word_vec2:\n",
    "                word_vec2[word] +=  doc_vec[line_id] \n",
    "            else:\n",
    "                word_vec2[word]=np.array([0.0]*dimensions)\n",
    "                #print(str(word_vec2[word]))\n",
    "\n",
    "                #doc_vec[line_id][0:18149]\n",
    "    line_id=line_id+1\n",
    "\n",
    "    \n",
    "#normalize all word vectors\n",
    "for k in word_vec2.keys():\n",
    "    word_vec2[k]=preprocessing.normalize(word_vec2[k].reshape(1,-1))\n",
    "\n",
    "print(\"word 2 doc\")  \n",
    "#and to doc vectors again\n",
    "doc_vec2=np.zeros((len(textz),dimensions))\n",
    "line_id=0\n",
    "for text in textz:\n",
    "    for word in re.split('[\\W]',text):\n",
    "        if word in nonsingular_wordz:\n",
    "            #document vector is a linear combination of IDF-weighted word vectors which document contains\n",
    "            try:\n",
    "                doc_vec2[line_id] = ( word_vec2[word] / (nonsingular_wordz[word]) ) + doc_vec2[line_id]\n",
    "            except:\n",
    "                1\n",
    "    line_id=line_id+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2word 2\n",
      "word 2 doc 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc_vec)):\n",
    "    doc_vec2[i]=preprocessing.normalize(doc_vec2[i].reshape(1,-1))\n",
    "\n",
    "    \n",
    "#from document vectors to word vectors\n",
    "print(\"doc2word 2\")\n",
    "word_vec2=dict()\n",
    "line_id=0\n",
    "for text in textz:\n",
    "    for word in re.split(' ',text):\n",
    "        if word in nonsingular_wordz:\n",
    "            #print(word)\n",
    "            if word in word_vec2:\n",
    "                word_vec2[word] +=  doc_vec2[line_id] \n",
    "            else:\n",
    "                word_vec2[word]=np.array([0.0]*dimensions)\n",
    "                #print(str(word_vec2[word]))\n",
    "\n",
    "                #doc_vec[line_id][0:18149]\n",
    "    line_id=line_id+1\n",
    "\n",
    "    \n",
    "#normalize all word vectors\n",
    "for k in word_vec2.keys():\n",
    "    word_vec2[k]=preprocessing.normalize(word_vec2[k].reshape(1,-1))\n",
    "\n",
    "print(\"word 2 doc 2\")  \n",
    "#and to doc vectors again\n",
    "doc_vec2=np.zeros((len(textz),dimensions))\n",
    "line_id=0\n",
    "for text in textz:\n",
    "    for word in re.split('[\\W]',text):\n",
    "        if word in nonsingular_wordz:\n",
    "            #document vector is a linear combination of IDF-weighted word vectors which document contains\n",
    "            try:\n",
    "                doc_vec2[line_id] = ( word_vec2[word] / (nonsingular_wordz[word]) ) + doc_vec2[line_id]\n",
    "            except:\n",
    "                1\n",
    "    line_id=line_id+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def nearest_neighbor(query_id,database):\n",
    "    query=database[query_id]\n",
    "    closest = distance.cdist([query], database, 'cosine')\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 geotechnik bodenmechanik buch vermitteln wichtig aspekte ueber aufbau eigenschaften bodens planung berechnung sowie begutachtung schaeden systems bauwerkbaugrund beruecksichtigen schwerpunkte baugrunderkundung ermittlung bodenkennwerten labor sowie behandlung setzungs tragfaehigkeitsnachweisen einschliesslich erddrucks unterstuetzung verstaendnisses dienen zahlreiche beispiele nachvollziehbar erlaeutert darstellungen basieren aktuell technisch regelwerk buch unverzichtbar orientierungshilfe taeglichen planungs gutachterpraxis\n",
      "2607 0.5344274399162543 äcker wirte gaben ländlich bodenmarkt liberale eigentumsordnung westfalen 19 jahrhunderts jahrhundert werden preussen baeuerliche eigentumsrechte schaffen entgegen annahmen reformer fuehrte marktgesteuerten bewegung faktors boden derjenige betrieben effizient nutzen acker wandern gut wirt vielmehr bleiben mobilitaet bodens einbetten system erbschaft familiaerer bergabe georg fertigen verfolgen buch bewegung parzellen hoefen landwirten dreier westfaelischer orte untersuchen einfluss lebenslauf verwandtschaftlichen beziehungen nachbarschaft kaufen verkaufen land zeigen logik marktes gabe sinne wirtschaftsanthropologie mobilitaet bodens bestimmen kern familiaerer umverteilung gepraegte baeuerliche gesellschaft liberal marktwirtschaft fortbestehen\n"
     ]
    }
   ],
   "source": [
    "import numpy.ma as ma\n",
    "\n",
    "query_id=5\n",
    "nns=nearest_neighbor(query_id,doc_vec)\n",
    "nn_id=np.argmin(ma.masked_where(nns<0.0000000001, nns))\n",
    "print(query_id,textz[query_id])\n",
    "print(nn_id,nns[0][nn_id],textz[nn_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Compute Cosine Similarity through dot product</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "nrm = Normalizer('l2')\n",
    "\n",
    "doc_vec_normed = nrm.fit_transform(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/after_nltk/data_after_nltk.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity:\n",
      "0.6897351250877295\n",
      "\n",
      "Title 1:\n",
      " schulbucharbeit das geschichtslehrbuch in der unterrichtspraxis\n",
      "\n",
      "Abstract 1:\n",
      "schulbucharbeit geschichtslehrbuch unterrichtspraxis trotzen zunehmend konkurrenz gelten schulbuch leitmedium geschichtsunterrichts manchmal behaupten modern schulbuecher selbstlaeufer verfuegen ueber zahlreiche methodenseiten bieten tipps unterrichtsgestaltung gut geschichtsunterricht moeglich lehrerinnen lehrer intensiv medial eigenlogik schulbuchs auseinandersetzen entstehen schulbuecher aufbauen zeichnen gutes schulbuch methoden schulbucharbeit eignen fuer reflektieren lehrlernprozesse vorliegend band geben zentrale fragen antwort erste umfassend monografie thema\n",
      "\n",
      "Title 2:\n",
      " guter geschichtsunterricht grundlagen erkenntnisse hinweise\n",
      "\n",
      "Abstract 2:\n",
      "gut geschichtsunterricht grundlagen erkenntnisse hinweise studie gut geschichtsunterricht grundlagen erkenntnisse hinweise liefern beitrag nutzerorientierten geschichtsdidaktischen unterrichtsforschung zentrum arbeit stehen alltaeglicher geschichtsunterricht institutionell verordnet zeitlich raeumlich festgelegt sowie abgeschlossen prozess dadurch kommen eigentlich kern schule debatten input outcome unterricht hintergrund geraten staerker blick fokussierung laesst gut geschichtsunterricht definieren identifizieren beschreiben analysieren\n"
     ]
    }
   ],
   "source": [
    "# choose example \n",
    "example = query_id\n",
    "title_1 = df['title'].iloc[example]\n",
    "abstract_1 = textz[example]\n",
    "# get example vector\n",
    "vec_1 = doc_vec_normed[example,:]\n",
    "\n",
    "# compute similarity\n",
    "sim = np.dot(doc_vec_normed[example+1:,:],vec_1)\n",
    "\n",
    "# find most similar document, see paper\n",
    "title_2 = df['title'].iloc[np.argmax(sim)+example+1]\n",
    "abstract_2 = textz[np.argmax(sim)+example+1]\n",
    "\n",
    "print('\\nSimilarity:')\n",
    "print(np.max(sim))\n",
    "print('\\nTitle 1:')\n",
    "print(title_1)\n",
    "print('\\nAbstract 1:')\n",
    "print(abstract_1)\n",
    "print('\\nTitle 2:')\n",
    "print(title_2)\n",
    "print('\\nAbstract 2:')\n",
    "print(abstract_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>K nearest Neighbors</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " geotechnik bodenmechanik\n",
      "\n",
      "\n",
      "[0.46557256]\n",
      " äcker wirte gaben ländlicher bodenmarkt und liberale eigentumsordnung im westfalen des 19 jahrhunderts\n",
      "[0.41464291]\n",
      " verkehrsmedizin fahreignung fahrsicherheit unfallrekonstruktion madea verkehrsmedizin 2a \n",
      "[0.32349903]\n",
      " bodenkundliches praktikum eine einführung in pedologisches arbeiten für ökologen land und forstwirte geo und umweltwissenschaftler\n",
      "[0.29180923]\n",
      " umgebungseinflüsse auf antennen aufbauhöhe bodenparameter topographie und vegetation\n",
      "[0.28118623]\n",
      " diagnostik schulischer lern und leistungsschwierigkeiten ein leitfaden\n",
      "[0.27117204]\n",
      " untersuchungen zur entwicklung eines ertragsmesssystems für mähwerke auf basis des leistungsbedarfs eines schwadversetzers\n",
      "[0.23591744]\n",
      " bewertung von boden und fels auf verklebungen und feinkornfreisetzung beim maschinellen tunnelvortrieb\n",
      "[0.23173468]\n",
      " wärmepumpen fehler vermeiden bei planung installation und betrieb\n",
      "[0.20348628]\n",
      " catia v5 einstieg und effektives arbeiten\n",
      "[0.19869564]\n",
      " byzanz historischkulturwissenschaftliches handbuch daim byzanz \n"
     ]
    }
   ],
   "source": [
    "# compares similarity of two abstracts through dot product\n",
    "\n",
    "# validation_vector1 \n",
    "validation_vector1 = query_id\n",
    "val_abstract_1 = textz[validation_vector1]\n",
    "# get validation_vector1\n",
    "val_vec_1 = doc_vec_normed[validation_vector1,:]\n",
    "\n",
    "# compute similarity of all entities\n",
    "vec_temp = []\n",
    "for vec in doc_vec_normed:\n",
    "    i = np.dot(val_vec_1,vec)\n",
    "    vec_temp.append(i)\n",
    "\n",
    "# turn list into DataFrame\n",
    "df_vec_temp = pd.DataFrame(vec_temp)\n",
    "\n",
    "# Create new DataFrame for sorted sim-values\n",
    "df_vec_temp_sorted = df_vec_temp.copy()\n",
    "df_vec_temp_sorted.sort_values(by=[0],inplace = True, ascending=False)\n",
    "\n",
    "# Get list of indices\n",
    "lst_vec_temp_sorted = df_vec_temp_sorted.iloc[1:11,:].index.values.tolist()\n",
    "lst_vec_temp_sorted\n",
    "\n",
    "print(df['title'].iloc[validation_vector1])\n",
    "print('\\n')\n",
    "\n",
    "for i in lst_vec_temp_sorted:\n",
    "#    print('\\n')\n",
    "    print(df_vec_temp.iloc[i].values)\n",
    "    print(df['title'].iloc[i])\n",
    "#     print(textz[i])\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/after_nltk/doc_vec-200.npy', doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
