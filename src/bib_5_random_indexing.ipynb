{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedLineDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import spacy \n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip,os,glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from langdetect import detect\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_vector(dimensions,seed):\n",
    "    arr = np.array([0] * (dimensions-seed) + [1] * int(seed/2) + [-1] * int(seed/2) )\n",
    "    np.random.shuffle(arr)\n",
    "    return(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "textz=[]\n",
    "word_vec=dict()\n",
    "word_docz=dict()\n",
    "dimensions=200\n",
    "seed=4\n",
    "\n",
    "with open('../data/after_nltk/df_content.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile) \n",
    "    for row in reader:\n",
    "        textz.append(row['content_nltk'].lower())\n",
    "        #if int(row['id'])>5000:\n",
    "        #    break\n",
    "        if int(row['id'])%1000==0:\n",
    "            print(row['id'])\n",
    "        current_doc_wordz=dict()\n",
    "        content=row['content_nltk'].lower()\n",
    "        for word in re.split('[\\W]', content):\n",
    "            if word not in current_doc_wordz:\n",
    "                current_doc_wordz[word]=1\n",
    "                try:\n",
    "                    word_docz[word]=word_docz[word]+1\n",
    "                except:\n",
    "                    word_docz[word]=1\n",
    "                    word_vec[word]=generate_random_vector(dimensions,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonsingular_wordz = dict((k, v) for k, v in word_docz.items() if v > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_factor=1/sum(nonsingular_wordz.values())\n",
    "for k in nonsingular_wordz:\n",
    "    nonsingular_wordz[k]*=normalize_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999994301"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(nonsingular_wordz.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18143"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec=np.ones((len(textz),dimensions))\n",
    "line_id=0\n",
    "for text in textz:\n",
    "    for word in re.split('[\\W]',text):\n",
    "        if word in nonsingular_wordz:\n",
    "            doc_vec[line_id] = ( word_vec[word] / (nonsingular_wordz[word]) ) + doc_vec[line_id]\n",
    "    line_id=line_id+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Instead of doc_vec[text_id] = doc_vec[text_id] + word_vec[word] You will simply have a SECOND PASS loop in which You'll do something like word_vec[word] = word_vec[word] + doc_vec[doc_id]</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning the document vectors back into wordvectors\n",
    "word_vec_loop =()\n",
    "for word in word_vec:\n",
    "    word_vec_loop[word] = word_vec[word] + doc_vec[line_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerunning the loop with new word_vectors\n",
    "doc_vec=np.ones((len(textz),dimensions))\n",
    "line_id=0\n",
    "for text in textz:\n",
    "    for word in re.split('[\\W]',text):\n",
    "        if word in nonsingular_wordz:\n",
    "            doc_vec_loop[line_id] = ( word_vec_loop[word] / (nonsingular_wordz[word]) ) + doc_vec[line_id]\n",
    "    line_id=line_id+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def nearest_neighbor(query_id,database):\n",
    "    query=database[query_id]\n",
    "    closest = distance.cdist([query], database, 'cosine')\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 welt lernen nie vergessen schopenhauer schopenhauers werk gehoert klassisch repertoire philosophie revolutionieren traditionell menschenbild abendlandes eroeffnete neue horizonte fragens forschens spierlings schopenhauerlexikon leben werk kolossal weltverneiners amuesant lehrreich einzeln stichwoertern aufbereiten afterweisheit ueber stachelschweine tod teufel fundgrube sowohl fuer einsteiger fuer kenner\n",
      "3830 0.5571404629775837 handbuch geben berblick ueber schopenhauers leben praesentiert wichtig werke werkgruppen legen philosophisch kontext dar entstehen ausfuehrlich wirkung philosophie literatur musik bildend kunst darstellen besonder hilfsmittel forschung bieten handbuch zudem seitenkonkordanz gaengigen werkausgaben schopenhauers beruecksichtigtdie auflage werden aktualisieren artikel anthropologie evolutionstheorie geometrie tierethik ergaenzt\n"
     ]
    }
   ],
   "source": [
    "import numpy.ma as ma\n",
    "\n",
    "query_id=420\n",
    "nns=nearest_neighbor(query_id,doc_vec)\n",
    "nn_id=np.argmin(ma.masked_where(nns<0.0000000001, nns))\n",
    "print(query_id,textz[query_id])\n",
    "print(nn_id,nns[0][nn_id],textz[nn_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Compute Cosine Similarity through dot product</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "nrm = Normalizer('l2')\n",
    "\n",
    "doc_vec_normed = nrm.fit_transform(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/after_nltk/data_after_nltk.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity:\n",
      "0.4388970199029248\n",
      "\n",
      "Title 1:\n",
      " Kleines Schopenhauer-Lexikon  / von Volker Spierling\n",
      "\n",
      "Abstract 1:\n",
      "welt lernen nie vergessen schopenhauer schopenhauers werk gehoert klassisch repertoire philosophie revolutionieren traditionell menschenbild abendlandes eroeffnete neue horizonte fragens forschens spierlings schopenhauerlexikon leben werk kolossal weltverneiners amuesant lehrreich einzeln stichwoertern aufbereiten afterweisheit ueber stachelschweine tod teufel fundgrube sowohl fuer einsteiger fuer kenner\n",
      "\n",
      "Title 2:\n",
      " Sinn  / von Gregor Bongaerts\n",
      "\n",
      "Abstract 2:\n",
      "studierend soziologie leserinnen allgemein geistes sozialwissenschaftlicher theoriebildung interessieren finden buch orientierung nahezu unuebersichtlichen gelaende soziologisch theorien sinnbegriffe vergleichend bersicht stellen einfuehrung zentrale sinnbegriffe basiskategorien verschieden soziologisch theorien verwenden abhaengigkeit konstruktion soziologisch gegenstandsbereichs theoretisch grundannahmen weisen beobachtbar zeigen zuletzt wahl sinnbegriffs entscheiden fuer moeglichkeiten soziologisch fragens methodisch beobachtens buch eignen hervorragend weisen studium lehre soziologie insbesondere einfuehrenden modulen soziologisch theorie strukturieren begleiten\n"
     ]
    }
   ],
   "source": [
    "# choose example \n",
    "example = query_id\n",
    "title_1 = df['title'].iloc[example]\n",
    "abstract_1 = textz[example]\n",
    "# get example vector\n",
    "vec_1 = doc_vec_normed[example,:]\n",
    "\n",
    "# compute similarity\n",
    "sim = np.dot(doc_vec_normed[example+1:,:],vec_1)\n",
    "\n",
    "# find most similar document, see paper\n",
    "title_2 = df['title'].iloc[np.argmax(sim)+example+1]\n",
    "abstract_2 = textz[np.argmax(sim)+example+1]\n",
    "\n",
    "print('\\nSimilarity:')\n",
    "print(np.max(sim))\n",
    "print('\\nTitle 1:')\n",
    "print(title_1)\n",
    "print('\\nAbstract 1:')\n",
    "print(abstract_1)\n",
    "print('\\nTitle 2:')\n",
    "print(title_2)\n",
    "print('\\nAbstract 2:')\n",
    "print(abstract_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>K nearest Neighbors</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18143, 200)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec_normed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Kleines Schopenhauer-Lexikon  / von Volker Spierling\n",
      "\n",
      "\n",
      "[0.43889702]\n",
      " Sinn  / von Gregor Bongaerts\n",
      "[0.38823975]\n",
      " Schopenhauer-Handbuch  : Leben – Werk – Wirkung \n",
      "[0.36922715]\n",
      " Kunst, Raum, Autorschaft  : Der Nachlass des US-amerikanischen Malers C.H. Phillips (1889-1975) aus autorgeografischer Perspektive  / von Alexia Pooth\n",
      "[0.34487528]\n",
      " Angewandte Funktionalanalysis  : Funktionalanalysis, Sobolev-Räume und elliptische Differentialgleichungen  / von Manfred Dobrowolski\n",
      "[0.3145693]\n",
      " Einführung in die Geschichte der Soziologie  / von Hermann Korte\n",
      "[0.28757217]\n",
      " Was ist Literatur?  : Basistexte zur Literaturtheorie \n",
      "[0.27820936]\n",
      " Bildbegriff und Kunstverständnis im kunstpädagogischen Kontext  / von Kunibert Bering, Julia Gerber, Nadja Nafe, Rolf Niehoff, Karina Pauls\n",
      "[0.27285277]\n",
      " Kompendium Adoleszenzpsychiatrie  : Krankheitsbilder mit CME-Fragen \n",
      "[0.27210431]\n",
      " Der Einfluss der Erstsprache auf den Erwerb der Zweitsprache  : Eine empirische Untersuchung zum Einfluss erstsprachlicher Strukturen bei zweisprachig türkisch-deutschen, kroatisch-deutschen und griechisch-deutschen Hauptschülern und Gymnasiasten  / von Seda Tunç\n",
      "[0.27154212]\n",
      " Praxis Pädagogik  : Schulartübergreifend  Praxis Pädagogik / Inklusion  : Schulartübergreifend \n"
     ]
    }
   ],
   "source": [
    "# compares similarity of two abstracts through dot product\n",
    "\n",
    "# validation_vector1 \n",
    "validation_vector1 = query_id\n",
    "val_abstract_1 = textz[validation_vector1]\n",
    "# get validation_vector1\n",
    "val_vec_1 = doc_vec_normed[validation_vector1,:]\n",
    "\n",
    "# compute similarity of all entities\n",
    "vec_temp = []\n",
    "for vec in doc_vec_normed:\n",
    "    i = np.dot(val_vec_1,vec)\n",
    "    vec_temp.append(i)\n",
    "\n",
    "# turn list into DataFrame\n",
    "df_vec_temp = pd.DataFrame(vec_temp)\n",
    "\n",
    "# Create new DataFrame for sorted sim-values\n",
    "df_vec_temp_sorted = df_vec_temp.copy()\n",
    "df_vec_temp_sorted.sort_values(by=[0],inplace = True, ascending=False)\n",
    "\n",
    "# Get list of indices\n",
    "lst_vec_temp_sorted = df_vec_temp_sorted.iloc[1:11,:].index.values.tolist()\n",
    "lst_vec_temp_sorted\n",
    "\n",
    "print(df['title'].iloc[validation_vector1])\n",
    "print('\\n')\n",
    "\n",
    "for i in lst_vec_temp_sorted:\n",
    "#    print('\\n')\n",
    "    print(df_vec_temp.iloc[i].values)\n",
    "    print(df['title'].iloc[i])\n",
    "#     print(textz[i])\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('../data/after_nltk/doc_vec-200.npy', doc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 2: Generate our 2D embedding</h1>\n",
    "https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec = np.load('../data/after_nltk/doc_vec-200.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates TSNE model and plots it\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "\n",
    "def tsne_plot(message_embeddings):\n",
    "    labels = []\n",
    "    tokens = []  \n",
    "    \n",
    "    tsne_model = TSNE(perplexity=50, n_components=2, init='pca',\n",
    "                      n_iter=5000, \n",
    "                      random_state=23)\n",
    "\n",
    "    new_values = tsne_model.fit_transform(message_embeddings)\n",
    "\n",
    "    global x,y\n",
    "    \n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i],s=0.2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = pd.DataFrame.from_dict({'x':x,'y':y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/after_nltk/data_after_nltk.pkl')\n",
    "df_all = pd.concat([df, df_values], axis=1, sort=False)\n",
    "df_all = df_all.drop(['Languagereview', 'content_nltk'], axis=1)\n",
    "df_all.to_excel('../data/after_nltk/data_for_tableau_testing_p40.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
