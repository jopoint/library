{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy \n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip,os,glob\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_vector(dimensions,seed):\n",
    "    arr = np.array([0] * (dimensions-seed) + [1] * int(seed/2) + [-1] * int(seed/2) )\n",
    "    np.random.shuffle(arr)\n",
    "    return(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "textz=[]\n",
    "word_vec=dict()\n",
    "word_docz=dict()\n",
    "dimensions=500\n",
    "seed=8\n",
    "\n",
    "with open('../data/after_nltk/df_content_after_nltk.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile) \n",
    "    for row in reader:\n",
    "        textz.append(row['content'].lower())\n",
    "        #if int(row['id'])>5000:\n",
    "        #    break\n",
    "        if int(row['id'])%1000==0:\n",
    "            print(row['id'])\n",
    "        current_doc_wordz=dict()\n",
    "        content=row['content'].lower()\n",
    "        for word in re.split('[\\W]', content):\n",
    "            if word not in current_doc_wordz:\n",
    "                current_doc_wordz[word]=1\n",
    "                try:\n",
    "                    word_docz[word]=word_docz[word]+1\n",
    "                except:\n",
    "                    word_docz[word]=1\n",
    "                    word_vec[word]=generate_random_vector(dimensions,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonsingular_wordz = dict((k, v) for k, v in word_docz.items() if v > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_factor=1/sum(nonsingular_wordz.values())\n",
    "for k in nonsingular_wordz:\n",
    "    nonsingular_wordz[k]*=normalize_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000011249"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(nonsingular_wordz.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18145"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec=np.ones((len(textz),dimensions))\n",
    "line_id=0\n",
    "for text in textz:\n",
    "    for word in re.split('[\\W]',text):\n",
    "        if word in nonsingular_wordz:\n",
    "            doc_vec[line_id] = ( word_vec[word] / (nonsingular_wordz[word]) ) + doc_vec[line_id]\n",
    "    line_id=line_id+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Instead of doc_vec[text_id] = doc_vec[text_id] + word_vec[word] You will simply have a SECOND PASS loop in which You'll do something like word_vec[word] = word_vec[word] + doc_vec[doc_id]</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # returning the document vectors back into wordvectors\n",
    "# word_vec_loop =()\n",
    "# for word in word_vec:\n",
    "#     word_vec_loop[word] = word_vec[word] + doc_vec[line_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rerunning the loop with new word_vectors\n",
    "# doc_vec=np.ones((len(textz),dimensions))\n",
    "# line_id=0\n",
    "# for text in textz:\n",
    "#     for word in re.split('[\\W]',text):\n",
    "#         if word in nonsingular_wordz:\n",
    "#             doc_vec_loop[line_id] = ( word_vec_loop[word] / (nonsingular_wordz[word]) ) + doc_vec[line_id]\n",
    "#     line_id=line_id+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def nearest_neighbor(query_id,database):\n",
    "    query=database[query_id]\n",
    "    closest = distance.cdist([query], database, 'cosine')\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 klein schopenhauerlexikon welt lernen nie vergessen schopenhauer schopenhauers werk gehoert klassisch repertoire philosophie revolutionieren traditionell menschenbild abendlandes eroeffnete neue horizonte fragens forschens spierlings schopenhauerlexikon leben werk kolossal weltverneiners amuesant lehrreich einzeln stichwoertern aufbereiten afterweisheit ueber stachelschweine tod teufel fundgrube sowohl fuer einsteiger fuer kenner\n",
      "3811 0.5325149506867946 sinn studierend soziologie leserinnen allgemein geistes sozialwissenschaftlicher theoriebildung interessieren finden buch orientierung nahezu unuebersichtlichen gelaende soziologisch theorien sinnbegriffe vergleichend bersicht stellen einfuehrung zentrale sinnbegriffe basiskategorien verschieden soziologisch theorien verwenden abhaengigkeit konstruktion soziologisch gegenstandsbereichs theoretisch grundannahmen weisen beobachtbar zeigen zuletzt wahl sinnbegriffs entscheiden fuer moeglichkeiten soziologisch fragens methodisch beobachtens buch eignen hervorragend weisen studium lehre soziologie insbesondere einfuehrenden modulen soziologisch theorie strukturieren begleiten\n"
     ]
    }
   ],
   "source": [
    "import numpy.ma as ma\n",
    "\n",
    "query_id=420\n",
    "nns=nearest_neighbor(query_id,doc_vec)\n",
    "nn_id=np.argmin(ma.masked_where(nns<0.0000000001, nns))\n",
    "print(query_id,textz[query_id])\n",
    "print(nn_id,nns[0][nn_id],textz[nn_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Compute Cosine Similarity through dot product</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "nrm = Normalizer('l2')\n",
    "\n",
    "doc_vec_normed = nrm.fit_transform(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/after_nltk/data_after_nltk.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity:\n",
      "0.46748504931320534\n",
      "\n",
      "Title 1:\n",
      " kleines schopenhauerlexikon\n",
      "\n",
      "Abstract 1:\n",
      "klein schopenhauerlexikon welt lernen nie vergessen schopenhauer schopenhauers werk gehoert klassisch repertoire philosophie revolutionieren traditionell menschenbild abendlandes eroeffnete neue horizonte fragens forschens spierlings schopenhauerlexikon leben werk kolossal weltverneiners amuesant lehrreich einzeln stichwoertern aufbereiten afterweisheit ueber stachelschweine tod teufel fundgrube sowohl fuer einsteiger fuer kenner\n",
      "\n",
      "Title 2:\n",
      " sinn\n",
      "\n",
      "Abstract 2:\n",
      "sinn studierend soziologie leserinnen allgemein geistes sozialwissenschaftlicher theoriebildung interessieren finden buch orientierung nahezu unuebersichtlichen gelaende soziologisch theorien sinnbegriffe vergleichend bersicht stellen einfuehrung zentrale sinnbegriffe basiskategorien verschieden soziologisch theorien verwenden abhaengigkeit konstruktion soziologisch gegenstandsbereichs theoretisch grundannahmen weisen beobachtbar zeigen zuletzt wahl sinnbegriffs entscheiden fuer moeglichkeiten soziologisch fragens methodisch beobachtens buch eignen hervorragend weisen studium lehre soziologie insbesondere einfuehrenden modulen soziologisch theorie strukturieren begleiten\n"
     ]
    }
   ],
   "source": [
    "# choose example \n",
    "example = query_id\n",
    "title_1 = df['title'].iloc[example]\n",
    "abstract_1 = textz[example]\n",
    "# get example vector\n",
    "vec_1 = doc_vec_normed[example,:]\n",
    "\n",
    "# compute similarity\n",
    "sim = np.dot(doc_vec_normed[example+1:,:],vec_1)\n",
    "\n",
    "# find most similar document, see paper\n",
    "title_2 = df['title'].iloc[np.argmax(sim)+example+1]\n",
    "abstract_2 = textz[np.argmax(sim)+example+1]\n",
    "\n",
    "print('\\nSimilarity:')\n",
    "print(np.max(sim))\n",
    "print('\\nTitle 1:')\n",
    "print(title_1)\n",
    "print('\\nAbstract 1:')\n",
    "print(abstract_1)\n",
    "print('\\nTitle 2:')\n",
    "print(title_2)\n",
    "print('\\nAbstract 2:')\n",
    "print(abstract_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>K nearest Neighbors</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kleines schopenhauerlexikon\n",
      "\n",
      "\n",
      "[0.46748505]\n",
      " sinn\n",
      "[0.43790761]\n",
      " schopenhauerhandbuch leben werk wirkung \n",
      "[0.24350684]\n",
      " arthur schopenhauer zur einführung\n",
      "[0.236215]\n",
      " landschaftsblick und landschaftsbild wahrnehmung und ästhetik im reisebericht 17801820\n",
      "[0.20265953]\n",
      " handbuch wissenschaftsgeschichte sommer hb wissenschaftsgesch \n",
      "[0.19809495]\n",
      " studien zur visuellen kultur eine einführung\n",
      "[0.19750995]\n",
      " bus fahren vierfarbiges bilderbuch lautobus\n",
      "[0.19144006]\n",
      " gut aufgehoben museumsdepots planen und betreiben \n",
      "[0.17987943]\n",
      " der begriff der kultur kulturphilosophie als aufgabe \n",
      "[0.17699838]\n",
      " gedichte nach dem leben\n"
     ]
    }
   ],
   "source": [
    "# compares similarity of two abstracts through dot product\n",
    "\n",
    "# validation_vector1 \n",
    "validation_vector1 = query_id\n",
    "val_abstract_1 = textz[validation_vector1]\n",
    "# get validation_vector1\n",
    "val_vec_1 = doc_vec_normed[validation_vector1,:]\n",
    "\n",
    "# compute similarity of all entities\n",
    "vec_temp = []\n",
    "for vec in doc_vec_normed:\n",
    "    i = np.dot(val_vec_1,vec)\n",
    "    vec_temp.append(i)\n",
    "\n",
    "# turn list into DataFrame\n",
    "df_vec_temp = pd.DataFrame(vec_temp)\n",
    "\n",
    "# Create new DataFrame for sorted sim-values\n",
    "df_vec_temp_sorted = df_vec_temp.copy()\n",
    "df_vec_temp_sorted.sort_values(by=[0],inplace = True, ascending=False)\n",
    "\n",
    "# Get list of indices\n",
    "lst_vec_temp_sorted = df_vec_temp_sorted.iloc[1:11,:].index.values.tolist()\n",
    "lst_vec_temp_sorted\n",
    "\n",
    "print(df['title'].iloc[validation_vector1])\n",
    "print('\\n')\n",
    "\n",
    "for i in lst_vec_temp_sorted:\n",
    "#    print('\\n')\n",
    "    print(df_vec_temp.iloc[i].values)\n",
    "    print(df['title'].iloc[i])\n",
    "#     print(textz[i])\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('../data/after_nltk/doc_vec-200.npy', doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
