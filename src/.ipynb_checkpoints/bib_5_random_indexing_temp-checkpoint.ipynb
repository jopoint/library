{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedLineDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import spacy \n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip,os,glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from langdetect import detect\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_vector(dimensions,seed):\n",
    "    arr = np.array([0] * (dimensions-seed) + [1] * int(seed/2) + [-1] * int(seed/2) )\n",
    "    np.random.shuffle(arr)\n",
    "    return(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "textz=[]\n",
    "word_vec=dict()\n",
    "word_docz=dict()\n",
    "dimensions=200\n",
    "seed=4\n",
    "\n",
    "with open('../data/after_nltk/df_content.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile) \n",
    "    for row in reader:\n",
    "        textz.append(row['content_nltk'].lower())\n",
    "        #if int(row['id'])>5000:\n",
    "        #    break\n",
    "        if int(row['id'])%1000==0:\n",
    "            print(row['id'])\n",
    "        current_doc_wordz=dict()\n",
    "        content=row['content_nltk'].lower()\n",
    "        for word in re.split('[\\W]', content):\n",
    "            if word not in current_doc_wordz:\n",
    "                current_doc_wordz[word]=1\n",
    "                try:\n",
    "                    word_docz[word]=word_docz[word]+1\n",
    "                except:\n",
    "                    word_docz[word]=1\n",
    "                    word_vec[word]=generate_random_vector(dimensions,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonsingular_wordz = dict((k, v) for k, v in word_docz.items() if v > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_factor=1/sum(nonsingular_wordz.values())\n",
    "for k in nonsingular_wordz:\n",
    "    nonsingular_wordz[k]*=normalize_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999994301"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(nonsingular_wordz.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18143"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec=np.ones((len(textz),dimensions))\n",
    "line_id=0\n",
    "for text in textz:\n",
    "    for word in re.split('[\\W]',text):\n",
    "        if word in nonsingular_wordz:\n",
    "            doc_vec[line_id] = ( word_vec[word] / (nonsingular_wordz[word]) ) + doc_vec[line_id]\n",
    "    line_id=line_id+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in doc_vec:\n",
    "    vec[word] = word_vec[word] + doc_vec[line_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec=np.ones((len(textz),dimensions))\n",
    "line_id=0\n",
    "for text in textz:\n",
    "    for word in re.split('[\\W]',text):\n",
    "        if word in nonsingular_wordz:\n",
    "            word_vec[word] = word_vec[word] + doc_vec[line_id]\n",
    "    line_id=line_id+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doc_vec[text_id] = doc_vec[text_id] + word_vec[word] You will simply have a SECOND PASS loop in which You'll do something like word_vec[word] = word_vec[word] + doc_vec[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def nearest_neighbor(query_id,database):\n",
    "    query=database[query_id]\n",
    "    closest = distance.cdist([query], database, 'cosine')\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 welt lernen nie vergessen schopenhauer schopenhauers werk gehoert klassisch repertoire philosophie revolutionieren traditionell menschenbild abendlandes eroeffnete neue horizonte fragens forschens spierlings schopenhauerlexikon leben werk kolossal weltverneiners amuesant lehrreich einzeln stichwoertern aufbereiten afterweisheit ueber stachelschweine tod teufel fundgrube sowohl fuer einsteiger fuer kenner\n",
      "3811 8.561225003367312e-07 studierend soziologie leserinnen allgemein geistes sozialwissenschaftlicher theoriebildung interessieren finden buch orientierung nahezu unuebersichtlichen gelaende soziologisch theorien sinnbegriffe vergleichend bersicht stellen einfuehrung zentrale sinnbegriffe basiskategorien verschieden soziologisch theorien verwenden abhaengigkeit konstruktion soziologisch gegenstandsbereichs theoretisch grundannahmen weisen beobachtbar zeigen zuletzt wahl sinnbegriffs entscheiden fuer moeglichkeiten soziologisch fragens methodisch beobachtens buch eignen hervorragend weisen studium lehre soziologie insbesondere einfuehrenden modulen soziologisch theorie strukturieren begleiten\n"
     ]
    }
   ],
   "source": [
    "import numpy.ma as ma\n",
    "\n",
    "query_id=420\n",
    "nns=nearest_neighbor(query_id,doc_vec)\n",
    "nn_id=np.argmin(ma.masked_where(nns<0.0000000001, nns))\n",
    "print(query_id,textz[query_id])\n",
    "print(nn_id,nns[0][nn_id],textz[nn_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Compute Cosine Similarity through dot product</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "nrm = Normalizer('l2')\n",
    "\n",
    "doc_vec_normed = nrm.fit_transform(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/after_nltk/data_after_nltk.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity:\n",
      "0.9999991438774997\n",
      "\n",
      "Title 1:\n",
      " Kleines Schopenhauer-Lexikon  / von Volker Spierling\n",
      "\n",
      "Abstract 1:\n",
      "welt lernen nie vergessen schopenhauer schopenhauers werk gehoert klassisch repertoire philosophie revolutionieren traditionell menschenbild abendlandes eroeffnete neue horizonte fragens forschens spierlings schopenhauerlexikon leben werk kolossal weltverneiners amuesant lehrreich einzeln stichwoertern aufbereiten afterweisheit ueber stachelschweine tod teufel fundgrube sowohl fuer einsteiger fuer kenner\n",
      "\n",
      "Title 2:\n",
      " Sinn  / von Gregor Bongaerts\n",
      "\n",
      "Abstract 2:\n",
      "studierend soziologie leserinnen allgemein geistes sozialwissenschaftlicher theoriebildung interessieren finden buch orientierung nahezu unuebersichtlichen gelaende soziologisch theorien sinnbegriffe vergleichend bersicht stellen einfuehrung zentrale sinnbegriffe basiskategorien verschieden soziologisch theorien verwenden abhaengigkeit konstruktion soziologisch gegenstandsbereichs theoretisch grundannahmen weisen beobachtbar zeigen zuletzt wahl sinnbegriffs entscheiden fuer moeglichkeiten soziologisch fragens methodisch beobachtens buch eignen hervorragend weisen studium lehre soziologie insbesondere einfuehrenden modulen soziologisch theorie strukturieren begleiten\n"
     ]
    }
   ],
   "source": [
    "# choose example \n",
    "example = query_id\n",
    "title_1 = df['title'].iloc[example]\n",
    "abstract_1 = textz[example]\n",
    "# get example vector\n",
    "vec_1 = doc_vec_normed[example,:]\n",
    "\n",
    "# compute similarity\n",
    "sim = np.dot(doc_vec_normed[example+1:,:],vec_1)\n",
    "\n",
    "# find most similar document, see paper\n",
    "title_2 = df['title'].iloc[np.argmax(sim)+example+1]\n",
    "abstract_2 = textz[np.argmax(sim)+example+1]\n",
    "\n",
    "print('\\nSimilarity:')\n",
    "print(np.max(sim))\n",
    "print('\\nTitle 1:')\n",
    "print(title_1)\n",
    "print('\\nAbstract 1:')\n",
    "print(abstract_1)\n",
    "print('\\nTitle 2:')\n",
    "print(title_2)\n",
    "print('\\nAbstract 2:')\n",
    "print(abstract_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>K nearest Neighbors</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18143, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec_normed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Kleines Schopenhauer-Lexikon  / von Volker Spierling\n",
      "\n",
      "\n",
      "[2.70893552e+18]\n",
      " Materials Selection in Mechanical Design: Das Original mit Übersetzungshilfen  : Easy-Reading-Ausgabe  / von Michael F Ashby\n",
      "[2.66839244e+18]\n",
      " Yvonne Rainer  : Raum Körper Sprache/Space Body Language \n",
      "[2.6303419e+18]\n",
      " Luise von Preußen (1776-1810)  : Musik, Tanz und Literatur im Leben einer Königin.  / von Walter Salmen\n",
      "[2.61896258e+18]\n",
      " The Life of an Architect … and what he leaves behind  / von Mike (Maaik) Hermans\n",
      "[2.61415301e+18]\n",
      " Grenzgänge. Gender, Ethnizität und Klasse als Wissenskategorien der Musikwissenschaften \n",
      "[2.52567326e+18]\n",
      " Yoko Ono. Hans Ulrich Obrist  / von Hans U Obrist, Yoko Ono\n",
      "[2.42307274e+18]\n",
      " Prinzip Schönheit  : Ästhetik als kognitive Welterschließung in Natur, Design und Psychologie.  / von Rolf L. A. Küster\n",
      "[2.40100073e+18]\n",
      " Haegue Yang. Arrivals  / von Haegue Yang\n",
      "[2.37434151e+18]\n",
      " Dictionary of Non-Destructive Testing<br>Wörterbuch der Zerstörungsfreien Prüfung  : English-German/ Englisch-Deutsch<br>German-English/ Deutsch-Englisch<br>  / von Jochen Mußmann, Heinz Peter Schmitz\n",
      "[2.33404164e+18]\n",
      " Familiale Erziehungskompetenzen  : Beziehungsklima und Erziehungsleistung in der Familie als Problem und Aufgabe \n"
     ]
    }
   ],
   "source": [
    "# compares similarity of two abstracts through dot product\n",
    "\n",
    "# validation_vector1 \n",
    "validation_vector1 = query_id\n",
    "val_abstract_1 = textz[validation_vector1]\n",
    "# get validation_vector1\n",
    "val_vec_1 = doc_vec_normed[validation_vector1,:]\n",
    "\n",
    "# compute similarity of all entities\n",
    "vec_temp = []\n",
    "for vec in doc_vec_normed:\n",
    "    i = np.dot(val_vec_1,vec)\n",
    "    vec_temp.append(i)\n",
    "\n",
    "# turn list into DataFrame\n",
    "df_vec_temp = pd.DataFrame(vec_temp)\n",
    "\n",
    "# Create new DataFrame for sorted sim-values\n",
    "df_vec_temp_sorted = df_vec_temp.copy()\n",
    "df_vec_temp_sorted.sort_values(by=[0],inplace = True, ascending=False)\n",
    "\n",
    "# Get list of indices\n",
    "lst_vec_temp_sorted = df_vec_temp_sorted.iloc[1:11,:].index.values.tolist()\n",
    "lst_vec_temp_sorted\n",
    "\n",
    "print(df['title'].iloc[validation_vector1])\n",
    "print('\\n')\n",
    "\n",
    "for i in lst_vec_temp_sorted:\n",
    "#    print('\\n')\n",
    "    print(df_vec_temp.iloc[i].values)\n",
    "    print(df['title'].iloc[i])\n",
    "#     print(textz[i])\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('../data/after_nltk/doc_vec-200.npy', doc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 2: Generate our 2D embedding</h1>\n",
    "https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec = np.load('../data/after_nltk/doc_vec-200.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates TSNE model and plots it\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "\n",
    "def tsne_plot(message_embeddings):\n",
    "    labels = []\n",
    "    tokens = []  \n",
    "    \n",
    "    tsne_model = TSNE(perplexity=50, n_components=2, init='pca',\n",
    "                      n_iter=5000, \n",
    "                      random_state=23)\n",
    "\n",
    "    new_values = tsne_model.fit_transform(message_embeddings)\n",
    "\n",
    "    global x,y\n",
    "    \n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i],s=0.2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = pd.DataFrame.from_dict({'x':x,'y':y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/after_nltk/data_after_nltk.pkl')\n",
    "df_all = pd.concat([df, df_values], axis=1, sort=False)\n",
    "df_all = df_all.drop(['Languagereview', 'content_nltk'], axis=1)\n",
    "df_all.to_excel('../data/after_nltk/data_for_tableau_testing_p40.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
