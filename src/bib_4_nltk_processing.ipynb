{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import spacy \n",
    "import matplotlib.pyplot as plt\n",
    "import gzip,os,glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import codecs\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20247, 5)\n"
     ]
    }
   ],
   "source": [
    "# load data from pkl file \n",
    "df = pd.read_pickle('../data/data_filtered contents.pkl')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds new collumn, detects language of column\n",
    "df['Languagereview'] = df['content'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delets all rows with the value \"en\"\n",
    "df = df[df.Languagereview == \"de\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resets index from 0, 4, 11, ... (random) to 0,1,2, ...\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_pickle('../data/df_movies_cleaning_2.pkl')\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing non-alphabetical symbols in dataframe, keeping umlauts\n",
    "df_movies['content'] = df_movies['content'].replace('[^a-zA-Z \\äöüß]', '', regex=True).str.replace(' +',' ').str.strip()\n",
    "# Transform umlauts\n",
    "df_movies['content'] = df_movies['content'].str.replace(chr(252), 'ue')\n",
    "df_movies['content'] = df_movies['content'].str.replace(chr(228), 'ae')\n",
    "df_movies['content'] = df_movies['content'].str.replace(chr(246), 'oe')\n",
    "df_movies['content'] = df_movies['content'].str.replace(chr(196), 'Ae')\n",
    "df_movies['content'] = df_movies['content'].str.replace(chr(214), 'Oe')\n",
    "df_movies['content'] = df_movies['content'].str.replace(chr(220), 'Ue')\n",
    "df_movies['content'] = df_movies['content'].str.replace(chr(223), 'ss')\n",
    "# Lowercase\n",
    "df_movies['content'] = df_movies['content'].str.lower()\n",
    "# rename collumn\n",
    "df_movies.rename(columns={'title1': 'title'}, inplace=True)\n",
    "df_movies.rename(columns={'title3': 'author'}, inplace=True)\n",
    "# drop collumn\n",
    "df_movies.drop(['word_count'],1,inplace=True)\n",
    "df_movies.drop(['title2'],1,inplace=True)\n",
    "# add label collumn\n",
    "df_movies['medium'] ='movie'\n",
    "# add nan\n",
    "df_movies.fillna(value=pd.np.nan, inplace=True)\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing non-alphabetical symbols in dataframe, keeping umlauts\n",
    "df['content'] = df['content'].replace('[^a-zA-Z \\äöüß]', '', regex=True).str.replace(' +',' ').str.strip()\n",
    "# Transform umlauts\n",
    "df['content'] = df['content'].str.replace(chr(252), 'ue')\n",
    "df['content'] = df['content'].str.replace(chr(228), 'ae')\n",
    "df['content'] = df['content'].str.replace(chr(246), 'oe')\n",
    "df['content'] = df['content'].str.replace(chr(196), 'Ae')\n",
    "df['content'] = df['content'].str.replace(chr(214), 'Oe')\n",
    "df['content'] = df['content'].str.replace(chr(220), 'Ue')\n",
    "df['content'] = df['content'].str.replace(chr(223), 'ss')\n",
    "# Lowercase\n",
    "df['content'] = df['content'].str.lower()\n",
    "# add label collumn\n",
    "df['medium'] ='print'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Combining content</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].str.split('  / von').str[0]\n",
    "df['title'] = df['title'].str.replace('[^\\w\\s]','')\n",
    "df['title'] = df['title'].replace('\\s+', ' ', regex=True)\n",
    "df['title'] = df['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author'] = df['author'].str.split('  / von').str[0]\n",
    "df['author'] = df['author'].str.replace('[^\\w\\s]','')\n",
    "df['author'] = df['author'].replace('\\s+', ' ', regex=True)\n",
    "df['author'] = df['author'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df.copy()\n",
    "df_combined.drop(['class','word_count', 'Languagereview'],axis=1, inplace=True)\n",
    "df_combined.replace(np.nan, '', regex=True, inplace=True)\n",
    "df_combined.replace('\\s+', ' ', regex=True, inplace=True)\n",
    "\n",
    "df_combined['content_nltk'] = df_combined['title'] +\" \"+ df_combined['content']\n",
    "df_combined['content_nltk'] = df_combined['content_nltk'].str.strip()\n",
    "\n",
    "df_movies['title'] = df_movies['title'].str.lower()\n",
    "\n",
    "df_movies['content_nltk'] = df_movies['title'] +\" \"+ df_movies['content']\n",
    "df_movies['content_nltk'] = df_movies['content_nltk'].str.strip()\n",
    "\n",
    "# df['content_nltk'] = df_combined\n",
    "# df_movies['content_nltk'] = df_movies_combined\n",
    "\n",
    "# whitespaces to nan\n",
    "df_combined = df_combined.replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.replace('\\s+', ' ', regex=True, inplace=True)\n",
    "df_combined = pd.concat([df_combined, df_movies])\n",
    "df_combined.reset_index(inplace=True,drop=True)\n",
    "df_combined = df_combined[['title','author','content_nltk','medium']]\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Remove Stop Words</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grüne philosophie konservativer denkansatz umweltschutz liegt herkoemmlich hoheitsgebiet politischen linken bedrohung natur zufolge internationalen kapitalismus konsumverhalten unbegrenzten ausbeutung natuerlicher ressourcen zuzuschreiben roger scruton entbloesst grundverstaendnis gefaehrlichen trugschluss zerstoererisch kosysteme wirkt denen zukunft abhaengt konservative denker wuerdigt traditionelle prinzipien offensichtlichste wirksamste mittel erhalt planeten sichern lokalismus buergerverantwortung schlagen dabei bemuehungen schwerfaelliger nichtregierungsorganisationen internationaler komitees zukunft mitnichten gesichert roger scruton beweist gedankenreichen schrift fortbestand kosystems erde moeglich'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('german')\n",
    "\n",
    "df_combined['content_nltk'] = df_combined['content_nltk'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df_combined['content_nltk'].iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tokenizing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        aernout mik communitas aernout mik zaehlt wich...\n",
       "1        anton henning dt engl vorliegende publikation ...\n",
       "2        feuchtwangers familie tradition jüdisches selb...\n",
       "3        grüne philosophie konservativer denkansatz umw...\n",
       "4        technische dokumentation maschinen anlagenbau ...\n",
       "                               ...                        \n",
       "25579    jochen roller let's dance! lecture performance...\n",
       "25580    ai weiwei - stimmen verstummen chinesische kue...\n",
       "25581    new york tomorrow webdokumentation new york gi...\n",
       "25582    hostess hostess seit zwei jahren automechanike...\n",
       "25583    bruno peinado bruno peinado plastische kunst r...\n",
       "Name: content_nltk, Length: 25584, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tok = df_combined['content_nltk']\n",
    "df_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['aernout', 'mik', 'communitas', 'aernout', 'mik', 'zaehlt', 'wichtigsten', 'kuenstlern', 'niederlande', 'installationen', 'film', 'video', 'performance', 'skulptur', 'architektur', 'oszillieren', 'kunst', 'bewegten', 'bild', 'grundlegend', 'weiterentwickelt', 'werke', 'spiegeln', 'gesellschaften', 'kriege', 'wirtschaftliche', 'depression', 'wider', 'katalog', 'praesentiert', 'wichtige', 'arbeiten', 'kuenstlers', 'nie', 'zuvor', 'deutschland', 'sehen', 'ausstellung', 'oktober', 'januar', 'museum', 'folkwang', 'essen'], ['anton', 'henning', 'dt', 'engl', 'vorliegende', 'publikation', 'widmet', 'drei', 'ortsbezogenen', 'installationen', 'anton', 'henning', 'allesamt', 'drei', 'exponierten', 'orten', 'kunst', 'ausgefuehrt', 'frankfurter', 'salon', 'hans', 'hollein', 'erbauten', 'museum', 'fuer', 'moderne', 'kunst', 'mmk', 'frankfurt', 'main', 'oktogon', 'fuer', 'herford', 'frank', 'gehry', 'erbauten', 'erst', 'kuerzlich', 'eroeffneten', 'marta', 'herford', 'apotheotischen', 'antiphrasen', 'fuer', 'haus', 'esters', 'mies', 'van', 'rohe', 'erschaffenen', 'villa', 'esters', 'krefeld', 'installationen', 'teil', 'werke', 'frueheren', 'schaffensphasen', 'integriert', 'greift', 'fokus', 'ueber', 'aktuelle', 'produktion', 'hinaus', 'zieht', 'dergestalt', 'erstes', 'groesseres', 'resuemee', 'ueber', 'hennings', 'kuenstlerisches', 'schaffen', 'ganzen'], ['feuchtwangers', 'familie', 'tradition', 'jüdisches', 'selbstverständnis', 'jahre', 'geschichte', 'deutschjuedischen', 'buergertums', 'beispiel', 'traditionsreichen', 'sueddeutschen', 'unternehmerfamilie', 'feuchtwanger', 'stammgaeste', 'hofbraeuhaus', 'fuehlten', 'alpen', 'hause', 'liebten', 'theater', 'museen', 'stadt', 'pflegten', 'landesuebliche', 'feindschaft', 'gegenueber', 'preussen', 'muenchen', 'galt', 'berliner', 'jude', 'zugereister', 'ber', 'drei', 'generationen', 'verband', 'familie', 'feuchtwanger', 'strenge', 'juedische', 'orthodoxie', 'ausgepraegt', 'bayerischbarocken', 'lebensweise', 'beruehmtester', 'spross', 'schriftsteller', 'lion', 'feuchtwanger', 'hielt', 'berlin', 'suedfrankreich', 'kalifornien', 'sowohl', 'muenchner', 'mundart', 'selbstbewussten', 'judentum', 'fest', 'heike', 'specht', 'zeichnet', 'juedischen', 'familie', 'deutschland', 'jahrhundert', 'wirtschaftliche', 'etablierung', 'wirken', 'juedischen', 'gemeinde', 'erste', 'weltkrieg', 'revolution', 'goldenen', 'zwanziger', 'jahre', 'diskriminierung', 'verfolgung', 'nationalsozialismus', 'schliesslich', 'emigration', 'flucht', 'geschichte', 'feuchtwangers', 'geschichte', 'familienzusammenhalt', 'familienzwist', 'arrangierten', 'ehen', 'leidenschaftlicher', 'liebe', 'glaenzenden', 'erfolgen', 'bitteren', 'niederlagen'], ['grüne', 'philosophie', 'konservativer', 'denkansatz', 'umweltschutz', 'liegt', 'herkoemmlich', 'hoheitsgebiet', 'politischen', 'linken', 'bedrohung', 'natur', 'zufolge', 'internationalen', 'kapitalismus', 'konsumverhalten', 'unbegrenzten', 'ausbeutung', 'natuerlicher', 'ressourcen', 'zuzuschreiben', 'roger', 'scruton', 'entbloesst', 'grundverstaendnis', 'gefaehrlichen', 'trugschluss', 'zerstoererisch', 'kosysteme', 'wirkt', 'denen', 'zukunft', 'abhaengt', 'konservative', 'denker', 'wuerdigt', 'traditionelle', 'prinzipien', 'offensichtlichste', 'wirksamste', 'mittel', 'erhalt', 'planeten', 'sichern', 'lokalismus', 'buergerverantwortung', 'schlagen', 'dabei', 'bemuehungen', 'schwerfaelliger', 'nichtregierungsorganisationen', 'internationaler', 'komitees', 'zukunft', 'mitnichten', 'gesichert', 'roger', 'scruton', 'beweist', 'gedankenreichen', 'schrift', 'fortbestand', 'kosystems', 'erde', 'moeglich'], ['technische', 'dokumentation', 'maschinen', 'anlagenbau', 'anforderungen', 'buch', 'gibt', 'anwender', 'entscheidenden', 'informationen', 'hilfen', 'fuer', 'anforderungsgerechte', 'technische', 'dokumentation', 'hand', 'liefert', 'kompakte', 'uebersichtliche', 'darstellung', 'fuer', 'erstellung', 'produktdokumentation', 'relevanten', 'zusammenhaenge', 'anforderungen', 'gesetzen', 'egrichtlinien', 'normen', 'neben', 'berblick', 'ueber', 'anforderungen', 'zeigt', 'titel', 'ansaetze', 'hilfsmittel', 'umsetzung', 'sowohl', 'bedeutung', 'maschinenrichtlinie', 'fuer', 'interne', 'externe', 'technische', 'dokumentation', 'erlaeutert', 'fragen', 'cekennzeichnung', 'beantwortet', 'band', 'richtet', 'insbesondere', 'verantwortliche', 'bereich', 'technische', 'dokumentation', 'z', 'b', 'geschaeftsfuehrer', 'technische', 'leiter', 'radakteure', 'konstrukteure', 'entwickler', 'cekoordinatoren', 'qualitaets', 'projektmanager', 'einkaeufer', 'inhalt', 'rechtliche', 'vorgaben', 'europaeischen', 'union', 'gesetze', 'verordnungen', 'rechtsprechung', 'maschinenrichtlinie', 'technische', 'dokumentation', 'normen', 'stand', 'technik', 'sicherheitshinweise', 'redaktionshandbuch', 'style', 'guide', 'anlagendokumentation', 'hersteller', 'betreiberpflichten', 'technische', 'dokumentation', 'unternehmen', 'dokumentationen', 'erstellen', 'liste', 'relevanter', 'normen', 'gesetze']]\n"
     ]
    }
   ],
   "source": [
    "tokenized = [ nltk.word_tokenize( str(sentence), language='german' ) for sentence in df_tok ]\n",
    "tokenized_test = tokenized[0:5]\n",
    "print(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tok = df.apply(lambda row: nltk.word_tokenize(row['content_nltk'], language='german'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tok_test = df_tok[:3]\n",
    "# df_tok_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lemmetization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-918b0a4a2226>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content_nltk'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_combined\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content_nltk'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'german'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5273\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "\n",
    "for i in tokenized:\n",
    "    for word in i:\n",
    "        word = nltk.sent_tokenize(w,language='german')\n",
    "\n",
    "# tokenized_sent = nltk.tokenize.word_tokenize(text,language='german')\n",
    "# print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "\n",
    "tagger = ht.HanoverTagger('morphmodel_ger.pgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-0e163d1a492e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[0mnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m      \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m      \u001b[0mnew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\HanTa\\HanoverTagger.py\u001b[0m in \u001b[0;36mtag_sent\u001b[1;34m(self, sent, taglevel, casesensitive)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag_sent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtaglevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasesensitive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sent_viterbi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcasesensitive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtaglevel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\HanTa\\HanoverTagger.py\u001b[0m in \u001b[0;36mtag_sent_viterbi\u001b[1;34m(self, sent, casesensitive)\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mcasesensitive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                \u001b[0mcs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m             \u001b[0mwprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcasesensitive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconditional\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwprobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'UNKNOWN'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwprobs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#This should not occur but can result from erong settings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                \u001b[0mwprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\HanTa\\HanoverTagger.py\u001b[0m in \u001b[0;36mtag_word\u001b[1;34m(self, word, cutoff, casesensitive, conditional)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[0mupcase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m             \u001b[0mp_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\HanTa\\HanoverTagger.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'^(`|``|´|´´|\\'|\\'\\')$'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[1;34m'\"'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36mmatch\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \"\"\"Try to apply the pattern at the start of the string, returning\n\u001b[0;32m    172\u001b[0m     a Match object, or None if no match was found.\"\"\"\n\u001b[1;32m--> 173\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfullmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "new = []\n",
    "for items in tokenized_test:\n",
    "  new.append([])\n",
    "  for item in items:\n",
    "     item = tagger.tag_sent(tokenized_test)\n",
    "     new[-1].append(item)\n",
    "\n",
    "\n",
    "# tags = tagger.tag_sent(tokenized_test)\n",
    "# print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [] \n",
    "sentences_tok = [nltk.tokenize.word_tokenize(sent) for sent in sentences]\n",
    "for sent in sentences_tok:\n",
    "    tags = tagger.tag_sent(sent) \n",
    "    nouns_from_sent = [lemma for (word,lemma,pos) in tags if pos != \"ZY\"]\n",
    "    nouns.extend(nouns_from_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unser', 'Text', 'sein', 'nicht', 'sehr', 'lang', '--', 'aber', 'zum', 'Schluß', 'schreiben', 'wir', 'doch', 'noch', 'mal', 'etwas', 'mehr', 'Code', 'und', 'machen', 'noch', 'mal', 'eine', 'nett', 'Statistik']\n",
      "['sein', 'schreiben', 'machen']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "df_combined[\"content_nltk\"] = df_combined[\"content_nltk\"].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        aernout mik communitas aernout mik zaehlt wich...\n",
       "1        anton henning dt engl vorliegend publikation w...\n",
       "2        feuchtwangers familie tradition jüdisch selbst...\n",
       "3        grüne philosophie konservativ denkansatz umwel...\n",
       "4        technische dokumentation maschinen anlagenbau ...\n",
       "                               ...                        \n",
       "25584    jochen roller let 's dance ! lecture performan...\n",
       "25585    ai weiwei - stimmen verstummen chinesische kue...\n",
       "25586    new york tomorrow webdokumentation new york ge...\n",
       "25587    hostess hostess seit zwei jahren automechanike...\n",
       "25588    bruno peinado bruno peinado plastische kunst r...\n",
       "Name: content_nltk, Length: 25589, dtype: object"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined[\"content_nltk\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Export</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import = pd.read_pickle('../data/data_filtered contents.pkl')\n",
    "\n",
    "#df_combined['content'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../data/after_nltk/data_movies_after_nltk.pkl')\n",
    "# df_content.to_csv('../data/after_nltk/df_content.csv', index=True)\n",
    "#df[\"content_nltk\"].to_csv('../data/after_nltk/df_content_after_nltk+titles.csv', index=True)\n",
    "df[\"content_nltk\"].to_csv('../data/after_nltk/df_movies_content_after_nltk.csv', index=True)\n",
    "# df.to_excel('../data/after_nltk/data_after_nltk.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #counts number of lines\n",
    "# fname = \"df_content.txt\"\n",
    "# count = 0\n",
    "# with open(fname, 'r') as f:\n",
    "#     for line in f:\n",
    "#         count += 1\n",
    "# print(\"Total number of lines is:\", count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
